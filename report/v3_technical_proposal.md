# ì‡¼íŠ¹í—ˆ (Short-Cut) v3.0 ê¸°ìˆ  ê³ ë„í™” ìˆ˜ì • ì œì•ˆì„œ

> **Author:** Senior Software Architect  
> **Date:** 2026-01-28  
> **Version:** 3.0 Enterprise Edition  

---

## Executive Summary

Short-Cut v2.0 (Antigravity)ì€ ê¸°ëŠ¥ êµ¬í˜„ì´ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜, ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ìƒìš© ì„œë¹„ìŠ¤ë¡œ ë°œì „ì‹œí‚¤ê¸° ìœ„í•´ ë‹¤ìŒ 4ê°€ì§€ í•µì‹¬ ì˜ì—­ì˜ ê³ ë„í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤:

| ì˜ì—­ | í˜„ì¬ ìƒíƒœ | ê°œì„  ëª©í‘œ | ìš°ì„ ìˆœìœ„ |
|------|----------|----------|---------|
| ì•„í‚¤í…ì²˜ | In-memory FAISS (10K í•œê³„) | ë¶„ì‚° Vector DB + Hybrid Search | ğŸ”´ Critical |
| UX/ì„±ëŠ¥ | ë™ê¸° ì‘ë‹µ (10-30ì´ˆ ëŒ€ê¸°) | LLM Streaming + ì‹¤ì‹œê°„ í”¼ë“œë°± | ğŸŸ¡ High |
| ë°ì´í„° ì²˜ë¦¬ | Regex ê¸°ë°˜ íŒŒì‹± | NLP ê¸°ë°˜ ê³„ì¸µì  íŒŒì‹± | ğŸŸ¢ Medium |
| ë³´ì•ˆ | ì—†ìŒ | PII ë§ˆìŠ¤í‚¹ + Private LLM | ğŸ”´ Critical |

---

## 1. ì•„í‚¤í…ì²˜ ë³€ê²½ ì œì•ˆ

### 1.1 Vector DB í™•ì¥ì„± ê°œì„ 

#### AS-IS (í˜„ì¬)
```python
# vector_db.py
class FaissClient:
    def __init__(self):
        self.index = faiss.IndexFlatIP(1536)  # In-memory
        self.metadata = {}  # Python dict
```
- **í•œê³„**: 
  - RAM ì¢…ì† (32GB ê¸°ì¤€ ~300K ë²¡í„° í•œê³„)
  - ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤, ìˆ˜í‰ í™•ì¥ ë¶ˆê°€
  - ì„œë²„ ì¬ì‹œì‘ ì‹œ ì¸ë±ìŠ¤ ì¬ìƒì„± í•„ìš”

#### TO-BE (ê°œì„ )
```python
# vector_db.py (Option A: Milvus)
class MilvusClient:
    def __init__(self):
        connections.connect(
            alias="default",
            host=os.environ.get("MILVUS_HOST", "localhost"),
            port=19530,
        )
        self.collection = Collection("patents")
    
    async def search(self, query_embedding, top_k=10):
        # Distributed search across shards
        return self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param={"metric_type": "IP", "nprobe": 16},
            limit=top_k,
        )

# vector_db.py (Option B: Pinecone - Serverless)
class PineconeClient:
    def __init__(self):
        pinecone.init(
            api_key=os.environ["PINECONE_API_KEY"],
            environment="us-west4-gcp"
        )
        self.index = pinecone.Index("patents")
```

| í•­ëª© | FAISS | Milvus | Pinecone |
|------|-------|--------|----------|
| ë²¡í„° ìˆ˜ | ~300K | ìˆ˜ì‹­ì–µ+ | ìˆ˜ì‹­ì–µ+ |
| ìš´ì˜ | ì§ì ‘ ê´€ë¦¬ | Docker/K8s | ì„œë²„ë¦¬ìŠ¤ |
| ë¹„ìš© | ë¬´ë£Œ | ì¸í”„ë¼ ë¹„ìš© | $0.025/GB/ì›” |
| ì¶”ì²œ ìƒí™© | POC/ë°ëª¨ | On-premise | í´ë¼ìš°ë“œ |

**ì˜ˆìƒ ê³µìˆ˜**: L (2-3ì£¼)  
**ê¸°ìˆ  ë‚œì´ë„**: â­â­â­ (3/5)

---

### 1.2 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Hybrid Search) ë„ì…

#### AS-IS (í˜„ì¬)
```python
# patent_agent.py
async def hyde_search(self, user_idea):
    query_embedding = await self.embed_text(hypothetical_claim)
    results = await self.faiss_client.search(query_embedding)  # Dense only
```
- **í•œê³„**: ì˜ë¯¸ì  ìœ ì‚¬ì„±ë§Œ ê³ ë ¤, ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ëˆ„ë½

#### TO-BE (ê°œì„ )
```python
# vector_db.py - RRF (Reciprocal Rank Fusion)
def rrf_fusion(dense_results, sparse_results, k=60):
    """RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ Dense + Sparse ê²°ê³¼ ìœµí•©"""
    scores = defaultdict(float)
    
    for rank, result in enumerate(dense_results):
        scores[result.chunk_id] += 1 / (k + rank + 1)
    
    for rank, result in enumerate(sparse_results):
        scores[result.chunk_id] += 1 / (k + rank + 1)
    
    return sorted(scores.items(), key=lambda x: x[1], reverse=True)

# patent_agent.py
async def hybrid_search(self, user_idea, top_k=10):
    # 1. Dense search (FAISS/Milvus)
    dense_results = await self.vector_search(query_embedding)
    
    # 2. Sparse search (BM25)
    keywords = await self.extract_keywords(user_idea)
    sparse_results = self.bm25_search(keywords)
    
    # 3. RRF Fusion
    fused_results = rrf_fusion(dense_results, sparse_results)
    
    return fused_results[:top_k]
```

**ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨**:
```mermaid
flowchart LR
    Q[User Query] --> KE[Keyword Extraction]
    Q --> EMB[OpenAI Embedding]
    
    KE --> BM25[BM25 Search]
    EMB --> FAISS[FAISS Search]
    
    BM25 --> RRF[RRF Fusion]
    FAISS --> RRF
    
    RRF --> R[Ranked Results]
```

**ì˜ˆìƒ ê³µìˆ˜**: M (1ì£¼)  
**ê¸°ìˆ  ë‚œì´ë„**: â­â­ (2/5)

---

## 2. UX/ì„±ëŠ¥ ìµœì í™”

### 2.1 LLM Streaming Response

#### AS-IS (í˜„ì¬)
```python
# patent_agent.py
async def critical_analysis(self, user_idea, results):
    response = await self.client.chat.completions.create(
        model=ANALYSIS_MODEL,
        messages=[...],
        stream=False  # ì „ì²´ ì‘ë‹µ ëŒ€ê¸°
    )
    return CriticalAnalysisResponse(**json.loads(response.content))
```
- **í•œê³„**: 10-30ì´ˆ ë¶„ì„ ë™ì•ˆ ì‚¬ìš©ì ëŒ€ê¸° (ë¹ˆ í™”ë©´)

#### TO-BE (ê°œì„ )
```python
# patent_agent.py
async def critical_analysis_stream(self, user_idea, results):
    """Async Generatorë¡œ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°"""
    response = await self.client.chat.completions.create(
        model=ANALYSIS_MODEL,
        messages=[...],
        stream=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
    )
    
    async for chunk in response:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

# app.py
async def run_analysis_streaming(user_idea, container):
    agent = PatentAgent()
    
    # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
    with container.empty() as output:
        full_response = ""
        async for token in agent.critical_analysis_stream(user_idea, results):
            full_response += token
            output.markdown(full_response + "â–Œ")  # ì»¤ì„œ íš¨ê³¼
        
        output.markdown(full_response)  # ìµœì¢… ì¶œë ¥
```

**Streamlit í†µí•©**:
```python
# app.py - st.write_stream í™œìš©
with st.status("ë¶„ì„ ì¤‘...", expanded=True):
    st.write_stream(agent.critical_analysis_stream(user_idea, results))
```

**ì˜ˆìƒ ê³µìˆ˜**: S (3-5ì¼)  
**ê¸°ìˆ  ë‚œì´ë„**: â­â­ (2/5)

---

### 2.2 ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ ì•ˆì •ì„±

#### AS-IS (í˜„ì¬)
```python
# app.py
result = asyncio.run(run_analysis(user_idea, status_container))
```
- **í•œê³„**: asyncio.run()ì€ Streamlit ì´ë²¤íŠ¸ ë£¨í”„ì™€ ì¶©ëŒ ê°€ëŠ¥

#### TO-BE (ê°œì„ )
```python
# app.py
import nest_asyncio
nest_asyncio.apply()  # ì¤‘ì²© ì´ë²¤íŠ¸ ë£¨í”„ í—ˆìš©

# ë˜ëŠ” ThreadPoolExecutor ì‚¬ìš©
from concurrent.futures import ThreadPoolExecutor

executor = ThreadPoolExecutor(max_workers=4)

def run_async_task(coro):
    loop = asyncio.new_event_loop()
    try:
        return loop.run_until_complete(coro)
    finally:
        loop.close()

result = executor.submit(run_async_task, run_analysis(user_idea)).result()
```

**ì˜ˆìƒ ê³µìˆ˜**: S (2-3ì¼)  
**ê¸°ìˆ  ë‚œì´ë„**: â­â­â­ (3/5)

---

## 3. ë°ì´í„° ì²˜ë¦¬ ê²¬ê³ ì„±

### 3.1 ì²­êµ¬í•­ íŒŒì‹± ê°œì„ 

#### AS-IS (í˜„ì¬)
```python
# preprocessor.py - ClaimParser
CLAIM_PATTERNS = [
    r'(?P<num>\d+)\.\s*(?P<text>...)',  # Regex ê¸°ë°˜
]

def _fallback_parse(self, claims_text):
    # ë‹¨ìˆœ ì¤„ ë²ˆí˜¸ ë¶„ë¦¬
    lines = claims_text.split('\n')
```
- **í•œê³„**: ë¹„ì •í˜• ë¬¸ì„œ, ë‹¤êµ­ì–´, ë³µì¡í•œ ë“¤ì—¬ì“°ê¸° ì²˜ë¦¬ ë¶ˆê°€

#### TO-BE (ê°œì„ )
```python
# preprocessor.py - ê³„ì¸µì  íŒŒì‹± + NLP Fallback
class EnhancedClaimParser:
    def __init__(self):
        # Spacy ì„¼í…ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Fallbackìš©)
        try:
            import spacy
            self.nlp = spacy.load("en_core_web_sm")
        except:
            self.nlp = None
    
    def parse_claims_text(self, claims_text) -> List[ParsedClaim]:
        # 1ì°¨: Regex íŒ¨í„´ ë§¤ì¹­
        claims = self._regex_parse(claims_text)
        if claims:
            return claims
        
        # 2ì°¨: ê³µë°±/ë“¤ì—¬ì“°ê¸° ê¸°ë°˜ êµ¬ì¡° ë¶„ì„
        claims = self._structure_based_parse(claims_text)
        if claims:
            return claims
        
        # 3ì°¨: NLP ê¸°ë°˜ ë¬¸ì¥ ê²½ê³„ ì¸ì‹
        if self.nlp:
            claims = self._nlp_fallback_parse(claims_text)
            if claims:
                return claims
        
        # 4ì°¨: ìµœì†Œ ë‹¨ìœ„ ë¶„ë¦¬ (Ultimate Fallback)
        return self._minimal_parse(claims_text)
    
    def _structure_based_parse(self, text):
        """ë“¤ì—¬ì“°ê¸°ì™€ ë²ˆí˜¸ ì²´ê³„ ê¸°ë°˜ ê³„ì¸µì  íŒŒì‹±"""
        lines = text.split('\n')
        claim_tree = []
        current_indent = 0
        
        for line in lines:
            indent = len(line) - len(line.lstrip())
            # ë²ˆí˜¸ íŒ¨í„´ ê°ì§€: 1., (1), Claim 1, etc.
            num_match = re.match(r'^[\s]*(?:Claim\s*)?[(\[]?(\d+)[.)\]:]?\s*(.*)$', line)
            if num_match:
                claim_num = int(num_match.group(1))
                claim_text = num_match.group(2)
                claim_tree.append({
                    'num': claim_num,
                    'indent': indent,
                    'text': claim_text,
                    'children': []
                })
        
        return self._flatten_tree(claim_tree)
```

**ì˜ˆìƒ ê³µìˆ˜**: M (1ì£¼)  
**ê¸°ìˆ  ë‚œì´ë„**: â­â­â­ (3/5)

---

## 4. í”„ë¼ì´ë²„ì‹œ ë° ë³´ì•ˆ

### 4.1 ë³´ì•ˆ ì•„í‚¤í…ì²˜

#### AS-IS (í˜„ì¬)
- ì‚¬ìš©ì ì•„ì´ë””ì–´ê°€ OpenAI APIë¡œ ì§ì ‘ ì „ì†¡
- ë¡œê¹…ì— ë¯¼ê° ì •ë³´ í¬í•¨ ê°€ëŠ¥
- ë°ì´í„° ì•”í˜¸í™” ì—†ìŒ

#### TO-BE (ê°œì„ )

```mermaid
flowchart TD
    subgraph Client["Client Layer"]
        UI[Streamlit UI]
    end
    
    subgraph Security["Security Layer"]
        PII[PII Masker]
        ENCRYPT[AES-256 Encryption]
        AUDIT[Audit Logger]
    end
    
    subgraph Processing["Processing Layer"]
        AGENT[Patent Agent]
        PRIVATE[Private LLM Option]
        OPENAI[OpenAI API]
    end
    
    UI --> PII
    PII --> ENCRYPT
    ENCRYPT --> AGENT
    AGENT --> PRIVATE
    AGENT --> OPENAI
    AGENT --> AUDIT
```

**êµ¬í˜„ ì½”ë“œ**:
```python
# security/pii_masker.py
import re
from typing import Tuple, Dict

class PIIMasker:
    """ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹ (ê°€ì—­ì )"""
    
    PATTERNS = {
        'company_name': r'\b(Inc\.|Corp\.|LLC|Ltd\.?)\b',
        'patent_number': r'\b(US|EP|WO|KR|JP|CN)[\d-]+[A-Z]?\d*\b',
        'email': r'\b[\w.-]+@[\w.-]+\.\w+\b',
        'phone': r'\b\d{2,4}[-.\s]?\d{3,4}[-.\s]?\d{4}\b',
    }
    
    def mask(self, text: str) -> Tuple[str, Dict[str, str]]:
        """í…ìŠ¤íŠ¸ì—ì„œ PII ë§ˆìŠ¤í‚¹, ë³µì› ë§µ ë°˜í™˜"""
        restore_map = {}
        masked_text = text
        
        for pii_type, pattern in self.PATTERNS.items():
            for i, match in enumerate(re.finditer(pattern, masked_text)):
                token = f"[{pii_type.upper()}_{i}]"
                restore_map[token] = match.group()
                masked_text = masked_text.replace(match.group(), token, 1)
        
        return masked_text, restore_map
    
    def unmask(self, text: str, restore_map: Dict[str, str]) -> str:
        """ë§ˆìŠ¤í‚¹ ë³µì›"""
        for token, original in restore_map.items():
            text = text.replace(token, original)
        return text

# security/private_llm.py
class PrivateLLMRouter:
    """Private LLM ë¼ìš°í„° (ë¯¼ê° ë°ì´í„°ìš©)"""
    
    def __init__(self):
        self.use_private = os.environ.get("USE_PRIVATE_LLM", "false").lower() == "true"
        self.private_endpoint = os.environ.get("PRIVATE_LLM_ENDPOINT")
    
    async def generate(self, messages, **kwargs):
        if self.use_private and self.private_endpoint:
            return await self._call_private_llm(messages, **kwargs)
        else:
            return await self._call_openai(messages, **kwargs)
```

**ì˜ˆìƒ ê³µìˆ˜**: L (2-3ì£¼)  
**ê¸°ìˆ  ë‚œì´ë„**: â­â­â­â­ (4/5)

---

## 5. êµ¬í˜„ ìš°ì„ ìˆœìœ„ ë¡œë“œë§µ

```mermaid
gantt
    title Short-Cut v3.0 Implementation Roadmap
    dateFormat  YYYY-MM-DD
    
    section Phase 1 - Core
    Hybrid Search (RRF)      :a1, 2026-02-01, 7d
    LLM Streaming            :a2, 2026-02-08, 5d
    Claim Parser Enhancement :a3, 2026-02-13, 7d
    
    section Phase 2 - Scale
    Vector DB Migration      :b1, 2026-02-20, 14d
    PII Masking              :b2, 2026-02-20, 10d
    
    section Phase 3 - Security
    Private LLM Integration  :c1, 2026-03-06, 14d
    Audit Logging            :c2, 2026-03-06, 7d
```

---

## 6. ìš”ì•½

| ê°œì„  í•­ëª© | ê³µìˆ˜ | ë‚œì´ë„ | ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ |
|----------|------|--------|--------------|
| Hybrid Search (RRF) | M | â­â­ | ê²€ìƒ‰ ì •í™•ë„ +30% |
| LLM Streaming | S | â­â­ | UX ë§Œì¡±ë„ â†‘â†‘ |
| Claim Parser ê°•í™” | M | â­â­â­ | ë°ì´í„° í’ˆì§ˆ â†‘ |
| Vector DB í™•ì¥ | L | â­â­â­ | 100ë§Œ+ íŠ¹í—ˆ ì§€ì› |
| PII ë§ˆìŠ¤í‚¹ | M | â­â­â­ | ë³´ì•ˆ ì»´í”Œë¼ì´ì–¸ìŠ¤ |
| Private LLM | L | â­â­â­â­ | ì—”í„°í”„ë¼ì´ì¦ˆ í•„ìˆ˜ |

---

*Prepared by: Senior Software Architect*  
*Short-Cut v3.0 Technical Proposal*
